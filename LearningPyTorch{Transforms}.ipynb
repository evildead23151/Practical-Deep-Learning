{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4zqF8UvfYj81rzdsM3+0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evildead23151/Practical-Deep-Learning/blob/main/LearningPyTorch%7BTransforms%7D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Transforms"
      ],
      "metadata": {
        "id": "pUk_SiuNlut-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
        "\n",
        "\n",
        "All TorchVision datasets have two parameters -transform to modify the features and target_transform to modify the labels - that accept callables containing the transformation logic. The torchvision.transforms module offers several commonly-used transforms out of the box.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To make these transformations, we use ToTensor and Lambda."
      ],
      "metadata": {
        "id": "JHGapOAwlyo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQmSSwGlUne",
        "outputId": "bf6f3efa-9dc3-4c3c-e054-3868386c977e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 22.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 348kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.24MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 8.52MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target_transform uses a Lambda function to convert each numeric label y (0–9) into a one-hot encoded vector of length 10 using torch.zeros().scatter_(). This is helpful when training models that require labels in one-hot format."
      ],
      "metadata": {
        "id": "eQZdmDlpon1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1. ToTensor()"
      ],
      "metadata": {
        "id": "Y4hzJgRkorgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2. Lambda Transforms"
      ],
      "metadata": {
        "id": "iZ8qdPpnovwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_transform = Lambda( lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ],
      "metadata": {
        "id": "ULsSiI2emdkR"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}